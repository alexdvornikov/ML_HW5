{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alex Dvornikov\n",
    "# HW5 \n",
    "# Neural Network \"From Scratch\"\n",
    "# April 19, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy import optimize\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Change Jupyter Notebook cell width \n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Layout (each line is a layer)\n",
    "layout = [\n",
    "    {\"in_size\": in_size, \"out_size\": 100, \"activation\": \"relu\"},\n",
    "    {\"in_size\": 100, \"out_size\": 100, \"activation\": \"relu\"},\n",
    "    {\"in_size\": 100, \"out_size\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "\n",
    "def setup_params(layout):\n",
    "    # number_of_layers = len(layout)\n",
    "    # Initiate parameters\n",
    "    params = {}\n",
    "    \n",
    "    # Iterate over the layers\n",
    "    for idx, layer in enumerate(layout):\n",
    "        # Start enumration from 1\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # Get the input and output sizes of the layers\n",
    "        layer_in_size = layer[\"in_size\"]\n",
    "        layer_out_size = layer[\"out_size\"]\n",
    "        \n",
    "        # Initiate W (matrix) and b (vector) \n",
    "        # Multiply by 0.1 to force small initial weights \n",
    "        params['W' + str(layer_idx)] = np.random.randn(layer_out_size, layer_in_size) * 0.1\n",
    "        params['b' + str(layer_idx)] = np.random.randn(layer_out_size, 1) * 0.1\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "# Partial derivatives for backprop\n",
    "# For more look inside the'bp_layer' function below\n",
    "def sigmoid_back(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_back(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;\n",
    "\n",
    "# 'A' stands for Activation and 'dA' is derivative respect to the activation \n",
    "# In our notes we called it 'x' for the first layer and 'h' for the subsequent layers.\n",
    "# For the top layer it was yhat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagate through a single layer \n",
    "def fp_layer(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    # Calculate input to the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # Select activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "        \n",
    "    # Return the output and input of the activation function\n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagate through all the layers \n",
    "def fp_all(X, params, layout):\n",
    "    # Initiate a dictionary for info needed for backprop\n",
    "    memory = {}\n",
    "    # X (vector) is the bottom layer \n",
    "    A_curr = X\n",
    "    \n",
    "    # Iterate over layers\n",
    "    for idx, layer in enumerate(layout):\n",
    "        # Number the layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # Get the activation output from the previous layer\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # Get the activation function, the weights matrix, and the bias vector for the current layer\n",
    "        activation_curr = layer[\"activation\"]\n",
    "        W_curr = params[\"W\" + str(layer_idx)]\n",
    "        b_curr = params[\"b\" + str(layer_idx)]\n",
    "        # Calculate the activation for the current layer\n",
    "        A_curr, Z_curr = fp_layer(A_prev, W_curr, b_curr, activation_curr)\n",
    "        \n",
    "        # Save\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # Return the output of the activation and a dictionary of the intermediate values\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagate through a single layer \n",
    "def bp_layer(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # Number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # Get the activation function\n",
    "    if activation is \"relu\":\n",
    "        back_activation_func = relu_back\n",
    "    elif activation is \"sigmoid\":\n",
    "        back_activation_func = sigmoid_back\n",
    "    \n",
    "    # Calculate its derivative\n",
    "    dZ_curr = back_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # Chain Rule\n",
    "    # Calcualte the derivatives respect to the W (matrix), b (vector), and A_prev (matrix)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagate through all the layers \n",
    "def bp_all(Y_hat, Y, memory, params, layout):\n",
    "    # Initiate a dictionary for the gradients \n",
    "    grads = {}\n",
    "    # Number of examples\n",
    "    m = Y.shape[1]\n",
    "    # Force the predictions and the labels to be of the same shape\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # Derivative of the loss respect to the prediction (part of the chain rule)\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    # Start from the end\n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(layout))):\n",
    "        # The layers are numbered from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # Get the activation function for the current layer\n",
    "        activation_curr = layer[\"activation\"]\n",
    "\n",
    "        dA_curr = dA_prev\n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = bp_layer(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation_curr)\n",
    "        \n",
    "        grads[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(params, grads, layout, learning_rate):\n",
    "    # Gradient descent \n",
    "\n",
    "    # Iterate over the layers\n",
    "    for layer_idx, layer in enumerate(layout, 1):\n",
    "        params[\"W\" + str(layer_idx)] -= learning_rate * grads[\"dW\" + str(layer_idx)]        \n",
    "        params[\"b\" + str(layer_idx)] -= learning_rate * grads[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the binary cross entropy loss \n",
    "def calc_loss(Y_hat, Y):\n",
    "    # Number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    loss = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    # Remove extra brackets in the array\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "# Convert probabiliies into binary classes \n",
    "def prob_to_binary(probs):\n",
    "    # Copy probs to prob (a snapshot copy that won't change if probs changes)\n",
    "    prob = np.copy(probs)\n",
    "    prob[prob > 0.5] = 1\n",
    "    prob[prob <= 0.5] = 0\n",
    "    return prob\n",
    "\n",
    "# Get the accuracy \n",
    "def calc_accuracy(Y_hat, Y):\n",
    "    Y_hat_ = prob_to_binary(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, layout, epochs, learning_rate, verbose=True, callback=None):\n",
    "    # Initiate parameters \n",
    "    params = setup_params(layout)\n",
    "    # Initiate lists to track learning   \n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    # Iterate over epochs \n",
    "    for i in range(epochs):\n",
    "        # Step ahead\n",
    "        Y_hat, cache = fp_all(X, params, layout)\n",
    "        \n",
    "        # Get learning metrics and store them\n",
    "        loss = calc_loss(Y_hat, Y)\n",
    "        loss_list.append(loss)\n",
    "        accuracy = calc_accuracy(Y_hat, Y)\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "        # Backprop\n",
    "        grads = bp_all(Y_hat, Y, cache, params, layout)\n",
    "        # Update params \n",
    "        params = update(params, grads, layout, learning_rate)\n",
    "        \n",
    "        # Print learning metrics (every 50 epochs)\n",
    "        if(i % 50 == 0):\n",
    "            if(verbose): # Set verbose = True to print the metrics \n",
    "                print(\"Iteration: {:05} - loss: {:.5f} - accuracy: {:.5f}\".format(i, loss, accuracy))\n",
    "            if(callback is not None):\n",
    "                callback(i, params)\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test data\n",
    "\n",
    "# Download Tox21 data set.\n",
    "# !curl -LO http://bioinf.jku.at/research/DeepTox/tox21_dense_train.csv.gz\n",
    "# !gunzip tox21_dense_train.csv.gz \n",
    "# !curl -LO http://bioinf.jku.at/research/DeepTox/tox21_labels_train.csv.gz\n",
    "# !gunzip tox21_labels_train.csv.gz\n",
    "# !curl -LO http://bioinf.jku.at/research/DeepTox/tox21_dense_test.csv.gz\n",
    "# !curl -LO http://bioinf.jku.at/research/DeepTox/tox21_labels_test.csv.gz\n",
    "# !gunzip tox21_dense_test.csv.gz\n",
    "# !gunzip tox21_labels_test.csv.gz\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = pd.read_csv('tox21_dense_train.csv')\n",
    "X_train = np.array(X_train.iloc[:,1:])\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "in_size = X_train.shape[1]\n",
    "\n",
    "y_train = pd.read_csv('tox21_labels_train.csv')\n",
    "y_train = y_train.fillna(0)\n",
    "y_train = np.array(y_train.loc[:,'NR.AR']) # Each Column is a type of toxicity.\n",
    "\n",
    "X_test = pd.read_csv('tox21_dense_test.csv')\n",
    "X_test = np.array(X_test.iloc[:,1:])\n",
    "X_test = scaler.transform(X_test) # Must use exact same preprocessing as train set.\n",
    "\n",
    "y_test = pd.read_csv('tox21_labels_test.csv')\n",
    "y_test = y_test.fillna(0)\n",
    "y_test = np.array(y_test.loc[:,'NR.AR']) \n",
    "\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 - loss: 1.12800 - accuracy: 0.10605\n",
      "Iteration: 00050 - loss: 0.10593 - accuracy: 0.96940\n",
      "Iteration: 00100 - loss: 0.09695 - accuracy: 0.97247\n",
      "Iteration: 00150 - loss: 0.09206 - accuracy: 0.97430\n",
      "Iteration: 00200 - loss: 0.08847 - accuracy: 0.97438\n",
      "Iteration: 00250 - loss: 0.08538 - accuracy: 0.97512\n",
      "Iteration: 00300 - loss: 0.08262 - accuracy: 0.97537\n",
      "Iteration: 00350 - loss: 0.08005 - accuracy: 0.97579\n",
      "Iteration: 00400 - loss: 0.07757 - accuracy: 0.97612\n",
      "Iteration: 00450 - loss: 0.07517 - accuracy: 0.97653\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "params = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))), layout, 500, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.98\n",
      "Test set AUC: 0.77021\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "Y_test_hat, _ = fp_all(np.transpose(X_test), params, layout)\n",
    "\n",
    "# Accuracy on the test set\n",
    "acc_test = accuracy(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
    "\n",
    "\n",
    "pred = Y_test_hat.reshape(y_test.shape)\n",
    "auc_test = roc_auc_score(y_test, pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc_test))\n",
    "print(\"Test set AUC: %3.5f\" % (auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC Curve \n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)\n",
    "plt.plot(fpr, tpr, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set AUC: 0.73491\n"
     ]
    }
   ],
   "source": [
    "# Compare to neural network from Sklearn \n",
    "# The network \"from scratch\" can outperform the MLP classifier in Sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,100), random_state=1, max_iter=100)\n",
    "\n",
    "\n",
    "# Evaluate on test set.\n",
    "model.fit(X_train, y_train)\n",
    "p_te = model.predict_proba(X_test)\n",
    "auc_te = roc_auc_score(y_test, p_te[:, 1])\n",
    "print(\"Test set AUC: %3.5f\" % (auc_te))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
